{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing.\n",
    "\n",
    "Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language data (wikipedia).\n",
    "This rapidly improving area of artificial intelligence covers tasks such as speech recognition, natural-language understanding, and natural language generation.\n",
    "In the following projects, we're going to be building a strong NLP foundation by practicing:\n",
    "- Tokenizing - Splitting sentences and words from the body of text.\n",
    "- Part of Speech tagging\n",
    "- Chunking\n",
    "\n",
    "So, Let's begin.\n",
    "\n",
    "## Importing the Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "3.4\n",
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "print(sys.version)\n",
    "print(nltk.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the nltk package installed, lets go over some basic natural language processing vocabulary:\n",
    "### Corpus -\n",
    "Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.\n",
    "### Lexicon -\n",
    "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
    "### Token -\n",
    "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.\n",
    "\n",
    "### Stop Words with NLTK:\n",
    "When using Natural Language Processing, our goal is to perform some analysis or processing so that a computer can respond to text appropriately.\n",
    "\n",
    "The process of converting data to something a computer can understand is referred to as \"pre-processing.\" One of the major forms of pre-processing is going to be filtering out useless data. In natural language processing, useless words (data), are referred to as stop words.\n",
    "### Stemming Words with NLTK:\n",
    "Stemming, which attempts to normalize sentences, is another preprocessing step that we can perform. In the english language, different variations of words and sentences often having the same meaning. Stemming is a way to account for these variations; furthermore, it will help us shorten the sentences and shorten our lookup. For example, consider the following sentence:\n",
    "- I was taking a ride on my horse.\n",
    "- I was riding my horse.\n",
    "\n",
    "These sentences mean the same thing, as noted by the same tense (-ing) in each sentence; however, that isn't intuitively understood by the computer. To account for all the variations of words in the english language, we can use the Porter stemmer, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have some text, we can train the PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets tokenize the sample_text using our trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define a function that will tag each tokenized word with a part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with NLTK\n",
    "Now that each word has been tagged with a part of speech, we can move onto chunking: grouping the words into meaningful clusters. The main goal of chunking is to group words into \"noun phrases\", which is a noun with any associated verbs, adjectives, or adverbs.\n",
    "\n",
    "The part of speech tags that were generated in the previous step will be combined with regular expressions, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # combine the part-of-speech tag with a regular expression\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # draw the chunks with nltk\n",
    "            # chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking with NLTK\n",
    "Sometimes there are words in the chunks that we don't won't, we can remove them using a process called chinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # The main difference here is the }{, vs. the {}. This means we're removing \n",
    "            # from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            # chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "One of the most common forms of chunking in natural language processing is called \"Named Entity Recognition.\" NLTK is able to identify people, places, things, locations, monetary figures, and more.\n",
    "\n",
    "There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.\n",
    "\n",
    "Here, with the option of binary = True, this means either something is a named entity, or not. There will be no further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            # namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "### Text classification using NLTK\n",
    "\n",
    "Now that we have covered the basics of preprocessing for Natural Language Processing, we can move on to text classification using simple machine learning classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 2000\n",
      "First Review: (['\"', 'jack', 'frost', ',', '\"', 'is', 'one', 'of', 'those', 'dumb', ',', 'corny', 'concoctions', 'that', 'attempts', 'to', 'be', 'a', 'heartwarming', 'family', 'film', ',', 'but', 'is', 'too', 'muddled', 'in', 'its', 'own', 'cliches', 'and', 'predictability', 'to', 'be', 'the', 'least', 'bit', 'touching', '.', 'this', 'does', 'not', 'come', 'as', 'a', 'surprise', ',', 'since', 'the', 'studio', 'that', 'made', 'it', 'is', 'warner', 'brothers', ',', 'who', 'is', 'on', 'a', 'current', 'streak', 'of', 'one', 'bad', 'film', 'after', 'the', 'other', '.', 'jack', 'frost', '(', 'michael', 'keaton', ')', 'is', 'a', 'struggling', 'middle', '-', 'aged', 'rock', 'musician', 'who', 'loves', 'his', 'wife', ',', 'gabby', '(', 'kelly', 'preston', ')', ',', 'and', '11', '-', 'year', '-', 'old', 'son', ',', 'charlie', '(', 'joseph', 'cross', ')', ',', 'but', 'doesn', \"'\", 't', 'spend', 'nearly', 'enough', 'time', 'with', 'them', '.', 'when', 'he', 'receives', 'a', 'call', 'from', 'a', 'music', 'label', 'that', 'wants', 'to', 'hear', 'him', 'play', ',', 'he', 'has', 'to', 'cancel', 'his', 'planned', 'family', 'outing', 'up', 'in', 'the', 'mountains', 'for', 'christmas', '.', 'halfway', 'there', ',', 'jack', 'has', 'second', 'thoughts', ',', 'but', 'on', 'his', 'way', 'back', 'home', ',', 'is', 'in', 'a', 'car', 'accident', 'and', 'dies', '.', 'switch', 'forward', 'a', 'year', ',', 'christmas', 'is', 'approaching', 'once', 'again', ',', 'and', 'charlie', 'and', 'gabby', 'are', 'still', 'having', 'a', 'difficult', 'time', 'coming', 'to', 'terms', 'with', 'jack', \"'\", 's', 'death', '.', 'when', 'charlie', 'begins', 'to', 'play', 'the', 'harmonica', 'his', 'father', 'gave', 'him', 'the', 'night', 'before', 'he', 'died', ',', 'the', 'snowman', 'outside', 'the', 'house', 'is', 'taken', 'over', 'by', 'jack', \"'\", 's', 'spirit', '.', 'jack', 'wants', 'to', 'spend', 'some', 'time', 'with', 'his', 'son', 'before', 'the', 'upcoming', 'warm', 'front', 'melts', 'him', ',', 'but', 'charlie', 'desperately', 'tries', 'to', 'prevent', 'his', 'melting', 'demise', '.', '\"', 'frosty', 'the', 'snowman', ',', '\"', 'is', 'a', 'classic', 'cartoon', ',', 'and', 'the', 'idea', 'of', 'a', 'snowman', 'that', 'is', 'alive', 'works', 'splendidly', 'when', 'animated', ',', 'but', 'as', 'a', 'live', '-', 'action', 'film', ',', 'it', 'doesn', \"'\", 't', 'work', 'at', 'all', '.', 'after', 'a', 'somewhat', 'promising', 'prologue', 'in', 'which', 'the', 'frost', 'family', 'is', 'established', ',', '\"', 'jack', 'frost', ',', '\"', 'quickly', 'goes', 'downhill', ',', 'especially', 'once', 'the', 'snowman', 'comes', 'into', 'play', '.', 'since', 'jack', 'has', 'been', 'deceased', 'for', 'a', 'whole', 'year', ',', 'you', 'would', 'think', 'there', 'would', 'be', 'many', 'questions', 'to', 'ask', 'him', ',', 'such', 'as', ',', '\"', 'what', 'happens', 'after', 'you', 'die', '?', '\"', 'or', ',', '\"', 'how', 'does', 'it', 'feel', 'to', 'be', 'a', 'snowman', '?', '\"', 'but', 'instead', ',', 'the', 'film', 'focuses', 'on', 'a', 'snowball', 'fight', 'subplot', 'and', 'an', 'inevitably', 'oversentimental', 'climax', 'that', 'could', 'be', 'telegraphed', 'before', 'i', 'even', 'sat', 'down', 'to', 'watch', 'the', 'movie', '.', 'the', 'performances', 'are', 'respectable', 'enough', ',', 'but', 'no', 'one', 'deserves', 'to', 'be', 'punished', 'by', 'appearing', 'in', 'a', 'silly', 'film', 'like', 'this', '.', 'michael', 'keaton', 'at', 'least', 'got', 'off', 'easy', ',', 'since', 'he', 'disappears', 'after', 'the', 'first', 'twenty', 'minutes', ',', 'but', 'what', 'exactly', 'does', 'he', 'think', 'he', 'is', 'doing', 'with', 'his', 'career', 'here', '?', 'i', 'have', 'always', 'liked', 'kelly', 'preston', '.', 'she', 'is', 'clearly', 'a', 'talented', ',', 'charismatic', 'actress', ',', 'but', 'has', 'never', 'been', 'given', 'a', 'good', 'role', 'in', 'her', 'life', ',', 'usually', 'having', 'to', 'settle', 'for', 'a', 'one', '-', 'dimensional', 'supporting', 'character', ',', 'as', 'in', ',', '1997', \"'\", 's', ',', '\"', 'nothing', 'to', 'lose', ',', '\"', 'and', ',', '\"', 'addicted', 'to', 'love', '.', '\"', 'joseph', 'cross', 'was', 'probably', 'the', 'highlight', 'in', 'the', 'cast', ',', 'since', 'he', 'believably', 'portrayed', 'a', 'boy', 'suffering', 'the', 'loss', 'of', 'a', 'parent', '.', 'in', 'one', 'of', 'the', 'only', 'subplots', 'that', 'actually', 'works', ',', 'due', 'to', 'its', 'wittiness', ',', 'henry', 'rollins', 'is', 'highly', 'amusing', 'as', 'a', 'hockey', 'coach', 'who', 'becomes', 'terrified', 'and', 'paranoid', 'after', 'seeing', 'the', 'live', 'snowman', '.', 'this', 'brief', 'hint', 'of', 'cleverness', 'is', 'pushed', 'to', 'the', 'side', ',', 'however', ',', 'by', 'the', 'tried', '-', 'and', '-', 'true', 'main', 'plot', 'at', 'hand', ',', 'which', 'is', 'the', 'sappy', 'story', 'of', 'a', 'father', 'and', 'son', '.', 'since', 'i', 'knew', 'what', 'was', 'going', 'to', 'happen', 'by', 'the', 'time', 'the', 'conclusion', 'came', 'around', ',', 'i', 'had', 'no', 'choice', 'but', 'to', 'sit', 'there', 'and', 'listen', 'to', 'painfully', 'insipid', ',', 'cringe', '-', 'inducing', 'lines', 'of', 'dialogue', '.', 'some', 'of', 'my', 'favorites', 'was', 'an', 'interaction', 'between', 'the', 'son', 'and', 'father', ':', '\"', 'you', 'da', 'man', ',', '\"', 'says', 'charlie', '.', '\"', 'no', ',', 'i', 'da', 'snowman', ',', '\"', 'replies', 'jack', '.', 'or', 'how', 'about', 'this', 'little', 'zinger', ',', 'coming', 'from', 'a', 'school', 'bully', 'that', 'miraculously', 'becomes', 'friendly', 'towards', 'charlie', 'and', 'tries', 'to', 'help', 'him', 'out', ':', '\"', 'snowdad', 'is', 'better', 'than', 'no', 'dad', '.', '\"', 'do', 'people', 'really', 'get', 'paid', 'in', 'hollywood', 'for', 'writing', 'pieces', 'of', 'trash', 'like', 'this', '?', 'the', 'snowman', ',', 'created', 'by', 'john', 'henson', \"'\", 's', 'creature', 'shop', ',', 'is', 'more', 'believable', 'than', 'the', 'snowman', 'from', 'last', 'year', \"'\", 's', 'unintentionally', 'hilarious', 'direct', '-', 'to', '-', 'video', 'horror', 'flick', ',', 'also', 'called', ',', '\"', 'jack', 'frost', ',', '\"', 'but', 'it', 'still', 'was', 'difficult', 'to', 'tell', 'if', 'it', 'was', 'a', 'person', 'in', 'a', 'suit', 'or', 'computer', 'effects', '.', 'either', 'way', ',', 'it', 'was', 'an', 'awful', 'lot', 'of', 'work', 'to', 'go', 'through', ',', 'just', 'to', 'come', 'up', 'with', 'a', 'final', 'product', 'as', 'featherbrained', 'as', 'this', 'project', '.', 'as', 'a', 'seasonal', 'holiday', 'picture', ',', '\"', 'jack', 'frost', ',', '\"', 'is', 'pretty', 'much', 'a', 'clunker', '.', 'a', 'better', 'christmas', 'film', 'from', 'this', 'year', 'is', ',', '\"', 'i', \"'\", 'll', 'be', 'home', 'for', 'christmas', '.', '\"', 'better', 'yet', ',', 'my', 'suggestion', 'would', 'be', 'to', 'stay', 'home', 'and', 'watch', 'a', 'quality', 'film', ',', 'such', 'as', ',', '\"', 'it', \"'\", 's', 'a', 'wonderful', 'life', ',', '\"', '\"', 'a', 'christmas', 'story', ',', '\"', 'or', ',', '\"', 'prancer', '.', '\"', '\"', 'jack', 'frost', ',', '\"', 'is', 'an', 'earnest', ',', 'but', 'severely', 'misguided', 'film', ',', 'and', 'children', ',', 'as', 'well', 'as', 'adults', ',', 'deserve', 'better', '.', 'i', 'doubt', 'they', 'would', 'want', 'to', 'see', 'a', 'movie', 'about', 'the', 'death', 'of', 'a', 'parent', ',', 'anyway', '.'], 'neg')\n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {}'.format(len(documents)))\n",
    "print('First Review: {}'.format(documents[1]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))\n",
    "print('The word happy: {}'.format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use the 4000 most common words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The find_features function will determine which of the 3000 word features are contained in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use an example from a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      ":\n",
      "two\n",
      "teen\n",
      "couples\n",
      "go\n",
      "to\n",
      "a\n",
      "church\n",
      "party\n",
      ",\n",
      "drink\n",
      "and\n",
      "then\n",
      "drive\n",
      ".\n",
      "they\n",
      "get\n",
      "into\n",
      "an\n",
      "accident\n",
      "one\n",
      "of\n",
      "the\n",
      "guys\n",
      "dies\n",
      "but\n",
      "his\n",
      "girlfriend\n",
      "continues\n",
      "see\n",
      "him\n",
      "in\n",
      "her\n",
      "life\n",
      "has\n",
      "nightmares\n",
      "what\n",
      "'\n",
      "s\n",
      "deal\n",
      "?\n",
      "watch\n",
      "movie\n",
      "\"\n",
      "sorta\n",
      "find\n",
      "out\n",
      "critique\n",
      "mind\n",
      "-\n",
      "fuck\n",
      "for\n",
      "generation\n",
      "that\n",
      "touches\n",
      "on\n",
      "very\n",
      "cool\n",
      "idea\n",
      "presents\n",
      "it\n",
      "bad\n",
      "package\n",
      "which\n",
      "is\n",
      "makes\n",
      "this\n",
      "review\n",
      "even\n",
      "harder\n",
      "write\n",
      "since\n",
      "i\n",
      "generally\n",
      "applaud\n",
      "films\n",
      "attempt\n",
      "break\n",
      "mold\n",
      "mess\n",
      "with\n",
      "your\n",
      "head\n",
      "such\n",
      "(\n",
      "lost\n",
      "highway\n",
      "&\n",
      "memento\n",
      ")\n",
      "there\n",
      "are\n",
      "good\n",
      "ways\n",
      "making\n",
      "all\n",
      "types\n",
      "these\n",
      "folks\n",
      "just\n",
      "didn\n",
      "t\n",
      "snag\n",
      "correctly\n",
      "seem\n",
      "have\n",
      "taken\n",
      "pretty\n",
      "neat\n",
      "concept\n",
      "executed\n",
      "terribly\n",
      "so\n",
      "problems\n",
      "well\n",
      "its\n",
      "main\n",
      "problem\n",
      "simply\n",
      "too\n",
      "jumbled\n",
      "starts\n",
      "off\n",
      "normal\n",
      "downshifts\n",
      "fantasy\n",
      "world\n",
      "you\n",
      "as\n",
      "audience\n",
      "member\n",
      "no\n",
      "going\n",
      "dreams\n",
      "characters\n",
      "coming\n",
      "back\n",
      "from\n",
      "dead\n",
      "others\n",
      "who\n",
      "look\n",
      "like\n",
      "strange\n",
      "apparitions\n",
      "disappearances\n",
      "looooot\n",
      "chase\n",
      "scenes\n",
      "tons\n",
      "weird\n",
      "things\n",
      "happen\n",
      "most\n",
      "not\n",
      "explained\n",
      "now\n",
      "personally\n",
      "don\n",
      "trying\n",
      "unravel\n",
      "film\n",
      "every\n",
      "when\n",
      "does\n",
      "give\n",
      "me\n",
      "same\n",
      "clue\n",
      "over\n",
      "again\n",
      "kind\n",
      "fed\n",
      "up\n",
      "after\n",
      "while\n",
      "biggest\n",
      "obviously\n",
      "got\n",
      "big\n",
      "secret\n",
      "hide\n",
      "seems\n",
      "want\n",
      "completely\n",
      "until\n",
      "final\n",
      "five\n",
      "minutes\n",
      "do\n",
      "make\n",
      "entertaining\n",
      "thrilling\n",
      "or\n",
      "engaging\n",
      "meantime\n",
      "really\n",
      "sad\n",
      "part\n",
      "arrow\n",
      "both\n",
      "dig\n",
      "flicks\n",
      "we\n",
      "actually\n",
      "figured\n",
      "by\n",
      "half\n",
      "way\n",
      "point\n",
      "strangeness\n",
      "did\n",
      "start\n",
      "little\n",
      "bit\n",
      "sense\n",
      "still\n",
      "more\n",
      "guess\n",
      "bottom\n",
      "line\n",
      "movies\n",
      "should\n",
      "always\n",
      "sure\n",
      "before\n",
      "given\n",
      "password\n",
      "enter\n",
      "understanding\n",
      "mean\n",
      "showing\n",
      "melissa\n",
      "sagemiller\n",
      "running\n",
      "away\n",
      "visions\n",
      "about\n",
      "20\n",
      "throughout\n",
      "plain\n",
      "lazy\n",
      "!\n",
      "okay\n",
      "people\n",
      "chasing\n",
      "know\n",
      "need\n",
      "how\n",
      "giving\n",
      "us\n",
      "different\n",
      "offering\n",
      "further\n",
      "insight\n",
      "down\n",
      "apparently\n",
      "studio\n",
      "took\n",
      "director\n",
      "chopped\n",
      "themselves\n",
      "shows\n",
      "might\n",
      "ve\n",
      "been\n",
      "decent\n",
      "here\n",
      "somewhere\n",
      "suits\n",
      "decided\n",
      "turning\n",
      "music\n",
      "video\n",
      "edge\n",
      "would\n",
      "actors\n",
      "although\n",
      "wes\n",
      "bentley\n",
      "seemed\n",
      "be\n",
      "playing\n",
      "exact\n",
      "character\n",
      "he\n",
      "american\n",
      "beauty\n",
      "only\n",
      "new\n",
      "neighborhood\n",
      "my\n",
      "kudos\n",
      "holds\n",
      "own\n",
      "entire\n",
      "feeling\n",
      "unraveling\n",
      "overall\n",
      "doesn\n",
      "stick\n",
      "because\n",
      "entertain\n",
      "confusing\n",
      "rarely\n",
      "excites\n",
      "feels\n",
      "redundant\n",
      "runtime\n",
      "despite\n",
      "ending\n",
      "explanation\n",
      "craziness\n",
      "came\n",
      "oh\n",
      "horror\n",
      "slasher\n",
      "flick\n",
      "packaged\n",
      "someone\n",
      "assuming\n",
      "genre\n",
      "hot\n",
      "kids\n",
      "also\n",
      "wrapped\n",
      "production\n",
      "years\n",
      "ago\n",
      "sitting\n",
      "shelves\n",
      "ever\n",
      "whatever\n",
      "skip\n",
      "where\n",
      "joblo\n",
      "nightmare\n",
      "elm\n",
      "street\n",
      "3\n",
      "7\n",
      "/\n",
      "10\n",
      "blair\n",
      "witch\n",
      "2\n",
      "crow\n",
      "9\n",
      "salvation\n",
      "4\n",
      "stir\n",
      "echoes\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets do it for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can split the featuresets into training and testing datasets using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25)\n",
    "\n",
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use sklearn algorithms in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the testing dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 79.4\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
